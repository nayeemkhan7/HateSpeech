{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:/research work/2020-21 paper/lda/500dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>497</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I went to jail for a flock, &amp;amp; came home w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>498</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I will fuck u where u stand\". \"U dizzy bitch\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>499</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I woke up like thi-\" bitch go back to bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I would rather you call me bitch than sir\" -m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>501</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I'll beat on that pussy so hard\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0             0      3            0                   0        3      2   \n",
       "1             1      3            0                   3        0      1   \n",
       "2             2      3            0                   3        0      1   \n",
       "3             3      3            0                   2        1      1   \n",
       "4             4      6            0                   6        0      1   \n",
       "..          ...    ...          ...                 ...      ...    ...   \n",
       "489         497      3            0                   3        0      1   \n",
       "490         498      3            0                   3        0      1   \n",
       "491         499      3            0                   3        0      1   \n",
       "492         500      3            0                   3        0      1   \n",
       "493         501      3            0                   3        0      1   \n",
       "\n",
       "                                                 tweet  \n",
       "0    !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1    !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2    !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3    !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4    !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "..                                                 ...  \n",
       "489  \"I went to jail for a flock, &amp; came home w...  \n",
       "490  \"I will fuck u where u stand\". \"U dizzy bitch\"...  \n",
       "491         \"I woke up like thi-\" bitch go back to bed  \n",
       "492  \"I would rather you call me bitch than sir\" -m...  \n",
       "493                  \"I'll beat on that pussy so hard\"  \n",
       "\n",
       "[494 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>494.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>494.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>249.696356</td>\n",
       "      <td>3.155870</td>\n",
       "      <td>0.188259</td>\n",
       "      <td>2.556680</td>\n",
       "      <td>0.410931</td>\n",
       "      <td>1.101215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>145.108272</td>\n",
       "      <td>0.669531</td>\n",
       "      <td>0.504527</td>\n",
       "      <td>1.115912</td>\n",
       "      <td>0.954950</td>\n",
       "      <td>0.384642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>124.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>249.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>374.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>501.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       count  hate_speech  offensive_language     neither  \\\n",
       "count  494.000000  494.000000   494.000000          494.000000  494.000000   \n",
       "mean   249.696356    3.155870     0.188259            2.556680    0.410931   \n",
       "std    145.108272    0.669531     0.504527            1.115912    0.954950   \n",
       "min      0.000000    3.000000     0.000000            0.000000    0.000000   \n",
       "25%    124.250000    3.000000     0.000000            2.000000    0.000000   \n",
       "50%    249.000000    3.000000     0.000000            3.000000    0.000000   \n",
       "75%    374.750000    3.000000     0.000000            3.000000    0.000000   \n",
       "max    501.000000    9.000000     3.000000            7.000000    6.000000   \n",
       "\n",
       "            class  \n",
       "count  494.000000  \n",
       "mean     1.101215  \n",
       "std      0.384642  \n",
       "min      0.000000  \n",
       "25%      1.000000  \n",
       "50%      1.000000  \n",
       "75%      1.000000  \n",
       "max      2.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither',\n",
       "       'class', 'tweet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['class'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=df.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords,\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \\\n",
    "                        \"vader compound\", \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494, 1052)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l2\",C=0.01))\n",
    "X_ = select.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight='balanced',penalty='l2',C=0.01).fit(X_,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93        14\n",
      "           1       1.00      0.97      0.98       416\n",
      "           2       0.84      1.00      0.91        64\n",
      "\n",
      "    accuracy                           0.97       494\n",
      "   macro avg       0.91      0.99      0.94       494\n",
      "weighted avg       0.98      0.97      0.97       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
